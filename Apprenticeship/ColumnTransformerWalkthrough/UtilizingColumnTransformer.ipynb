{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing SKLearn's `ColumnTransformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point, we've done steps like scaling, imputing, and encoding separately, each as their own piece. But SKLearn has a handy class designed to preprocess groups of columns effectively, streamlining your pre-processing steps!\n",
    "\n",
    "Enter: [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "[Insurance Costs data](https://www.kaggle.com/mirichoi0218/insurance) (they got the idea for cleaning up the original open source data from [Machine Learning with R](https://www.packtpub.com/product/machine-learning-with-r-third-edition/9781788295864))\n",
    "\n",
    "Goal: predict insurance charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/insurance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our X and y\n",
    "X = df.drop(columns='charges')\n",
    "y = df['charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Our Steps\n",
    "\n",
    "Observations from our initial data exploration: \n",
    "- No null values\n",
    "- 6 input columns and 1 target column (`charges`)\n",
    "- `age`, `bmi`, and `children` are numeric\n",
    "- `sex`, `smoker`, and `region` are objects\n",
    "\n",
    "What preprocessing steps will we need to take?\n",
    "- Scale our three numeric inputs\n",
    "- Encode our three object inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Way\n",
    "\n",
    "We have a train test split, and want to avoid data leakage - if we pre-processed our data before splitting, we'd potenitally leak some information about the test set into our training data!\n",
    "\n",
    "Let's look at the steps we'd take to preprocess our data without data leakage and without using `ColumnTransformer`, scaling using SKLearn's `MinMaxScaler` and encoding using SKLearn's `OneHotEncoder`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tackle Our Object Columns First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of our object column names\n",
    "obj_cols = ['sex', 'smoker', 'region']\n",
    "\n",
    "# Separate our object columns for both train and test\n",
    "X_train_obj = X_train[obj_cols]\n",
    "X_test_obj = X_test[obj_cols]\n",
    "\n",
    "X_train_obj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit our encoder\n",
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "ohe.fit(X_train_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode our train and test sets\n",
    "X_train_ohe = ohe.transform(X_train_obj)\n",
    "X_test_ohe = ohe.transform(X_test_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Saving these as dataframes, with appropriate column names and index\n",
    "X_train_ohe = pd.DataFrame(X_train_ohe, \n",
    "                           columns = ohe.get_feature_names(input_features = obj_cols),\n",
    "                           index=X_train.index)\n",
    "X_test_ohe = pd.DataFrame(X_test_ohe, \n",
    "                          columns = ohe.get_feature_names(input_features = obj_cols),\n",
    "                          index=X_test.index)\n",
    "\n",
    "X_train_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Tackle Our Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of our numeric column names\n",
    "num_cols = ['age', 'bmi', 'children']\n",
    "\n",
    "# Separate our numeric columns for both train and test\n",
    "X_train_num = X_train[num_cols]\n",
    "X_test_num = X_test[num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit our scaler\n",
    "# using minmaxscaler because other cols are now binary\n",
    "scaler = MinMaxScaler() \n",
    "scaler.fit(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale our train and test sets\n",
    "X_train_scaled = scaler.transform(X_train_num)\n",
    "X_test_scaled = scaler.transform(X_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Saving these as dataframes, with appropriate column names and index\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, \n",
    "                              columns = num_cols,\n",
    "                              index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, \n",
    "                             columns = num_cols,\n",
    "                             index=X_test.index)\n",
    "\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put Them Back Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use concat\n",
    "X_train_processed = pd.concat([X_train_ohe, X_train_scaled], axis=1)\n",
    "X_test_processed = pd.concat([X_test_ohe, X_test_scaled], axis=1)\n",
    "\n",
    "X_train_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew! In order to do just these two preprocessing steps, we had to separate out our columns, process train and test the same (fitting on train then transforming both train and test of course), then put them back together - and to get all the names out we had to make everything dataframes to concat. That's a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Way!\n",
    "\n",
    "Let's see how `ColumnTransformer` simplifies the process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to import ColumnTransformer!\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've luckily already defined our list of columns\n",
    "# We'll need these for our ColumnTransformer\n",
    "print(f\"Object Columns: {obj_cols}\")\n",
    "print(f\"Numeric Columns: {num_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ColumnTransformer` takes a list of transformers, where each item in the list is a tuple with three parts:\n",
    "1. Nickname for the step (useful for getting out column names)\n",
    "2. Preprocessor object\n",
    "3. List of columns to transform with that preprocessor object\n",
    "\n",
    "> NOTE! Lists of columns used in `ColumnTransformer` must be mutually exclusive. If you put the same column name in multiple steps in a `ColumnTransformer`, you'll get multiple copies of that column each transformed in a different way.\n",
    "\n",
    "Let's show it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an columntransformer object\n",
    "ct = ColumnTransformer(transformers=[\n",
    "    ('ohe', OneHotEncoder(drop='first'), obj_cols),\n",
    "    ('scaler', MinMaxScaler(), num_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform\n",
    "ct.fit(X_train)\n",
    "X_train_ct = ct.transform(X_train)\n",
    "X_test_ct = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showcase the result - initial output is a numpy array\n",
    "X_train_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ever want to turn that resulting numpy array into a dataframe, we'll want to get out the column names after transformation - especially for preprocessing steps like `OneHotEncoder` which changes the number of columns.\n",
    "\n",
    "We can do that using that handy nickname we set when creating the `ColumnTransformer`! After the `ColumnTransformer` has been fit, we can access the `named_transformers_` attribute to access each step individiually, and treat it just like we would a normal preprocessor object on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the resulting column names from 'ohe'\n",
    "ohe_col_names = ct.named_transformers_['ohe'].get_feature_names(input_features = obj_cols)\n",
    "ohe_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's turn our X_train_ct into a dataframe to see\n",
    "# Note that steps are done in the order they're passed\n",
    "pd.DataFrame(X_train_ct,\n",
    "             columns = [*ohe_col_names, *num_cols], # Using * to unpack lists\n",
    "             index = X_train.index).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Easier or more work to use `ColumnTransformer`? Up to you - so long as you always avoid data leakage in your preprocessing steps!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
